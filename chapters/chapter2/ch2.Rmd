---
title: "ch2"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(vcd)
library(BiocManager)
library(Biostrings)
library(seqLogo)
library(HardyWeinberg)
library(BSgenome)
library(Renext)
library(Gviz)
library("BSgenome.Ecoli.NCBI.20080805")
library("BSgenome.Hsapiens.UCSC.hg19")
knitr::opts_chunk$set(echo = TRUE)
```


# Chapter 2 - Statistical Modeling

Statistical modelling is required when, unlike in chapter 1, the models and parameters and *unknown*. So to remedy that, these models and values are instead **estimated** using the data available to the statistician.  

The direction of information is from the **data upwards** through a process known as inference.  

## 2.1 Goals and Objectives

At the end of the chapter we will hopefully have a better idea about:  

-estimating models
-conducting a maximum likelihood simulation experiment 
-applying Bayesian statistics
-making Markov chain models  

*Parameters* 
-lambda is the single parameter that defines a Poisson distribution
-mu is often used to represent the mean of the normal 
-theta is used to represent the total paramaters of a given distribution, for example in bionomial: theta = (n,p)

## 2.3 Statistical Models

In order to carry out a modelling procedure, we must first have the following:  
- a reasonable probability distribution
  - discerete variables could be looked through binomial multinomial, or Poisson lenses
  - continuous variables could be looked at through the lens of the normal

```{r}
load("/Users/jeremiahyarmie/Desktop/stat7160/data/e100.RData")
e99 = e100[-which.max(e100)] #remove the outlier
```

A visual identification of goodness-of-fit of your data to a candidate model can be done by various plotting strategies. For discrete data a barplot works great. 

Note: e99 is e100 with the outlier removed.

```{r}
barplot(table(e99), space = 0.8, col = "chartreuse4")
```

But for a more granular approach a *rootogram* can be used, which overlays theoretical values to the counts of a barplot.

```{r}
gf1 = goodfit( e99, "poisson")
rootogram(gf1, xlab = "", rect_gp = gpar(fill = "chartreuse4"))
```

Exercise: Generate 100 Poisson distributed numbers with lambda = 0.5 and draw their rootogram.

```{r}
poispointfive <- rpois(100, 0.5)
gf2 = goodfit(poispointfive, "poisson")
rootogram(gf1, xlab = "", rect_gp = gpar(fill = "chartreuse4"))
```

It seems likely that e99 follows a Poisson distribution. We'll now need to estimate lambda using the data to give us lambda_hat. This is done by chosing the value of lambda_hat that makes it most likely for us to see the data that we see, the *maximum likelihood estimator*.

We return to using e100, the presence of the outlier will skew our MLE slightly. If our conclusions are still statistically significant even when including such outliers, we know that our approach is robust. We are being conservative in doing so.

### Estimating the Poisson distribution  

First we tally our observations:

```{r}
table(e100)
```

Now we have to try and match this distribution using rpois.

```{r}
rpois(100, 3) %>%  table
```

Lambda of 3 gives us too many 2s, 3s, and more compared to our data.

```{r}
rpois(100, 5) %>%  table
```

We hardly see any 0s or 1s, 5 is much too high.

```{r}
rpois(100, 1) %>%  table
```

Lambda = 1 is pretty close...  

But this brute force trial-and-error system isn't ideal. Instead, we can use *likelihood functions* to find the closest approximation. This is done by calculating the probability of seeing **exactly** our data given the Poisson distribution is a value **m**.

P(58x0, 34x1, 7x2, 1x7 | Lambda is m) = P(0)^58 x P(1)^34 x P(2)^7 x P(7)^1

```{r}
prod(dpois(c(0, 1, 2, 7), lambda = 3) ^ (c(58, 34, 7, 1)))
```

Okay so... lambda = 5.

```{r}
prod(dpois(c(0, 1, 2, 7), lambda = 5) ^ (c(58, 34, 7, 1)))
```

That's worse and unlikely.

```{r}
prod(dpois(c(0, 1, 2, 7), lambda = 1) ^ (c(58, 34, 7, 1)))
```

That's better...

```{r}
prod(dpois(c(0, 1, 2, 7), lambda = 0.4) ^ (c(58, 34, 7, 1)))
```

Looking even better...

```{r}
prod(dpois(c(0, 1, 2, 7), lambda = 0.5) ^ (c(58, 34, 7, 1)))
```


It's standard to take the logarithm, when the logarithm is at its max, so too should be the probability. 


```{r}
loglikelihood  =  function(lambda, data = e100) {
  sum(log(dpois(data, lambda)))
}
```

loglikelihood is function that lets us plug in different values for lambda. Now we can call it using a spread of lambda values.

```{r}
plot_pois <- function(){ #making this a function because I'm lazy

lambdas = seq(0.05, 0.95, length = 100) #make a sequence of lambda inputs from 0.05-0.95
loglik = vapply(lambdas, loglikelihood, numeric(1)) #vapply iteratively applies the function in the second argument to all items in the vector in the first argument, and produces output in the type of the third argument
plot(lambdas, loglik, type = "l", col = "red", ylab = "", lwd = 2,
     xlab = expression(lambda))
}
plot_pois()
```

So our function has a maximum around 0.5 (as we already know).

Let's see that:

```{r}
plot_pois()
m0 = mean(e100)
abline(v = m0, col = "blue", lwd = 2)
abline(h = loglikelihood(m0), col = "purple", lwd = 2)
```

And what is the mean?

```{r}
m0
```

Interesting! So the presence of the outlier inflated the value of lambda by 0.05! 

Apparently there was a way to do this all along in R... and we've already called the function before! goodfit

```{r}
gf = goodfit(e100, "poisson")
names(gf) #this will show us all of the outputs of goodfit()
```

*par* contains the values of the fitted parameters, in this case lambda. The other information available includes:
  - observed - observed frequencies
  - count - corresponding counts
  - fitted - expected frequencies given fitted maximum likelihood
  - type - indicates the distribution fitted
  - method - fitting method: ML, MinChisq, or fixed
  - df - degrees of freedom
  
```{r}
gf$observed
gf$count
gf$fitted
gf$type
gf$method
gf$df
```

But what we care about is par...

```{r}
gf$par
```

## 2.4 Applying Maximum Likelihood approaches to Binomial Data

The following example looks at testing n = 120 male individuals for red-green colourblindness.

```{r}
cb <- c(1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
table(cb)
```

If you had to guess the value of p^, you would probably say 1/12. Which is also the MLE.

Let's plot various values of p^ and see what the visualization looks like.

```{r}
probs  =  seq(0, 0.3, by = 0.005)
likelihood = dbinom(sum(cb), prob = probs, size = length(cb))
plot(probs, likelihood, pch = 16, xlab = "probability of success",
       ylab = "likelihood", cex=0.6)
probs[which.max(likelihood)]
```
And we get 0.085, which is very close to 1/12.

## 2.5 Exploring Multinomial with Nucleotide Data

```{r}
staph <- readDNAStringSet("/Users/jeremiahyarmie/Desktop/stat7160/data/staphsequence.ffn.txt", "fasta")
```

Using [n] we can look at the nth gene in our DNA sequence:

```{r}
staph[1]
```
And we can get a frequency count:

```{r}
staph_nt_count <- letterFrequency(staph[[1]], letters = "ACGT", OR = 0)
#[[i]] extracts the sequence of the i-th gene as a DNAString, as opposed to the pair of single brackets [i], which return a DNAStringSet with just a single DNAString in it
```

```{r}
gene1length <- length(staph[[1]])
divnt <- function(x){x / gene1length}
nt_freq <- sapply(staph_nt_count, divnt)
nt_freq
```

The nulceotide distribution does not appear to be following a 1/4 * 1/4 * 1/4 * 1/4 pattern.

We can scan the first ten genes:

```{r}
letterFrq = vapply(staph, letterFrequency, FUN.VALUE = numeric(4),
         letters = "ACGT", OR = 0)
colnames(letterFrq) = paste0("gene", seq(along = staph))
tab10 = letterFrq[, 1:10]
computeProportions = function(x) { x/sum(x) }
prop10 = apply(tab10, 2, computeProportions)
round(prop10, digits = 2)
```

It does look rather consistent...

Let p0 be a vector showing the multinomail means for all nucleotides in the first ten genes.

```{r}
p0 = rowMeans(prop10)
p0
```

We can then use a Monte Carlo simulation to evaluate deviations from these probabilities.

First we need the nucleotide sums for all ten genes (or the gene lengths).

```{r}
cs = colSums(tab10)
cs
```

Then we can multiply these counts with the p0 mean probabilities to get our expected counts of each nucleotide.

```{r}
expectedtab10 = outer(p0, cs, FUN = "*")
round(expectedtab10)
```

I DON'T REALLY GET THIS NEXT PART

```{r}
randomtab10 = sapply(cs, function(s) { rmultinom(1, s, p0) } )
all(colSums(randomtab10) == cs)
```

```{r}
stat = function(obsvd, exptd = 20 * pvec) {
   sum((obsvd - exptd)^2 / exptd)
}
B = 1000
simulstat = replicate(B, {
  randomtab10 = sapply(cs, function(s) { rmultinom(1, s, p0) })
  stat(randomtab10, expectedtab10)
})
S1 = stat(tab10, expectedtab10)
sum(simulstat >= S1)
```

```{r}
hist(simulstat, col = "lavender", breaks = seq(0, 75, length.out=50))
abline(v = S1, col = "red")
abline(v = quantile(simulstat, probs = c(0.95, 0.99)),
       col = c("darkgreen", "blue"), lty = 2)
```

The probability of seeing a value as large as S1=70.1 is very small under the null model, happening 0 times in our 1000 simulations. Conclusion: the ten genes do not seem to come from the same multinomial model.

## 2.6 Chi-Squared Distribution

So what we previously did wasn't entirely pointless... but it was a long way around just using a test statistic.

The theoretical distribution of the simulstat statistic is called the *chi-squared distribution*.

The parameter is 30 (10 x (4-1)). There are 10 genes and 4-1 degrees of freedom.

How well the theory and simulation match up can be done using the visual goodness-of-fit tool the *(QQ) plot* or the **quantile-quantile plot**. 

### 2.6.1 

Compare the simulstat values and 1000 randomly generated chi-squared(thirty) random numbers by displaying them in histograms with 50 bins each.

```{r}
rchisq(1000, 30) %>% barplot

```

```{r}
qs = ppoints(100)
quantile(simulstat, qs)
quantile(qchisq(qs, df = 30), qs)
```

Chi-squared is both a test statistic, as well as a distribution in it's own right with parameter nu.

Now we can plot our Q-Q plot. If our two datasets align well, then we should expect a diagonal line (y = x)

```{r}
qqplot(qchisq(ppoints(B), df = 30), simulstat, main = "",
  xlab = expression(chi[nu==30]^2), asp = 1, cex = 0.5, pch = 16)
abline(a = 0, b = 1, col = "red")
```

Now that we are confident, we can use a chi-squared(30) distribution to compute p.

```{r}
1 - pchisq(S1, df = 30)
```

## 2.7 Chargaff's Rule

```{r}
load("/Users/jeremiahyarmie/Desktop/stat7160/data/ChargaffTable.RData")
ChargaffTable
```

A=T and C=G, and we can define the statistic (pg - pc)^2 + (pa - pt)^2

What happens if we look at Chargaff's tables assuming that Chargaff's rule wasn't in place?

```{r}
statChf = function(x){
  sum((x[, "C"] - x[, "G"])^2 + (x[, "A"] - x[, "T"])^2)
}
chfstat = statChf(ChargaffTable)
permstat = replicate(100000, {
     permuted = t(apply(ChargaffTable, 1, sample))
     colnames(permuted) = colnames(ChargaffTable)
     statChf(permuted)
})
pChf = mean(permstat <= chfstat)
pChf
```
```{r}
hist(permstat, breaks = 100, main = "", col = "lavender")
abline(v = chfstat, lwd = 2, col = "red")
```

### 2.7.1 Two categorical variables

When we are measuring two categorical variables, the cross table is known as a *contingency table*. 

```{r}
HairEyeColor[,, "Female"]
```

Looking at colour blindness...

```{r}
load("/Users/jeremiahyarmie/Desktop/stat7160/data/Deuteranopia.RData")
Deuteranopia
```

For two categorical variables, we can run a chi-squared test statistic to see if there is a statistically significant difference between rates of deuteranopia in men and women.

```{r}
chisq.test(Deuteranopia)
```

### 2.7.2 Hardy-Weinberg -- Special Multinomial

Recall: p + q = 1 (allele frequencies A or a) and p^2 + 2pq + q^2 = 1 (genotype frequencies AA Aa aA or aa).

```{r}
data("Mourant")
Mourant[214:216,]
```

To plot the log-likelihood for the Tahiti data...

```{r}
nMM = Mourant$MM[216]
nMN = Mourant$MN[216]
nNN = Mourant$NN[216]
loglik = function(p, q = 1 - p) {
  2 * nMM * log(p) + nMN * log(2*p*q) + 2 * nNN * log(q)
}
xv = seq(0.01, 0.99, by = 0.01)
yv = loglik(xv)
plot(x = xv, y = yv, type = "l", lwd = 2,
     xlab = "p", ylab = "log-likelihood")
imax = which.max(yv)
abline(v = xv[imax], h = yv[imax], lwd = 1.5, col = "blue")
abline(h = yv[imax], lwd = 1.5, col = "purple")
```

We can compute the genotype frequencies using the af function from the HardyWeinberg package.

```{r}
phat  =  af(c(nMM, nMN, nNN))
phat
```

```{r}
pMM   =  phat^2
pMM
qhat  =  1 - phat
qhat
```

```{r}
pHW = c(MM = phat^2, MN = 2*phat*qhat, NN = qhat^2)
sum(c(nMM, nMN, nNN)) * pHW
```

A visual evaluation of the goodness-of-fit of Hardy-Weinberg can be called using HWTernaryPlot.

```{r}
pops = c(1, 69, 128, 148, 192)
genotypeFrequencies = as.matrix(Mourant[, c("MM", "MN", "NN")])
HWTernaryPlot(genotypeFrequencies[pops, ],
        markerlab = Mourant$Country[pops],
        alpha = 0.0001, curvecols = c("red", rep("purple", 4)),
        mcex = 0.75, vertex.cex = 1)
```

How to read: the Hardy-Weinberg model is the red curve, the acceptance region is between the two purple lines. 

### 2.7.3 Sequence Motifs

Let's look at the position weight matrix (PWM) or position-specific scoring matrix (PSSM), of the Kozak motif by looking at the sequence logo graphic.

```{r}
load("/Users/jeremiahyarmie/Desktop/stat7160/data/kozak.RData")
kozak
```

```{r}
pwm = makePWM(kozak)
seqLogo(pwm, ic.scale = FALSE)
```

## 2.9 Markov Chains and Bayesian Thinking

Bayesian statistics centre the importance of *prior knowledge* in the statistical pipeline. This prior knowledge could help inform the value of estimated parameters. Bayesian statistics allows us to **update** or **shift** our knowledge about our estimated parameters based on other information.

The paradigm considers a *prior* and *posterior* distributions, the latter being formed after we collect and interperet other information.

If our prior is P(x) and we have some other data d,then posterior would be P(x | d) and their relationship can be described by the following:

P(x | d) = P(d | x) x P(x) / P(d)

### 2.9.1 Haplotypes

A haplotype is a collection of physically close alleles that are often inhereted together (are genetically linked)

The goal is to estimate the proportion of a Y-haplotype containing short tandem repeats.

```{r}
haplo6 = read.table("/Users/jeremiahyarmie/Desktop/stat7160/data/haplotype6.txt", header = T)
haplo6
```

Our goal is to find the proportion, theta, of the haplotype of interest, Y, in a population of interest.

### 2.9.2 Simulation

When we are looking at a parameter that is a proportion or probability between 0 and 1, it is convenient to use the *beta distribution*. If our prior is the belief that theta is beta and observe data in the form of n binomial trials, then our posterior theta have an updated beta distribution.

The distribution of Y due to a different distribution of theta is known as the *marginal distribution* of Y

```{r}
rtheta = rbeta(100000, 50, 350)
y = vapply(rtheta, function(th) {
  rbinom(1, prob=th, size=300)
}, numeric(1))
hist(y, breaks =50, col ="orange", main= "", xlab = "")
```

To calculate the posterior of theta, we fous on outcomes where Y was 40.

```{r}
thetaPostEmp = rtheta[ y == 40 ]
hist(thetaPostEmp, breaks = 40, col = "chartreuse4", main = "", probability = T, xlab = expression("posterior"~theta)) ## note the rest of this code didn't work for me because thetas doesn't exist
```

The mean of which is:

```{r}
mean(thetaPostEmp)
```

We can use Monte Carlo simulation to estimate the theta parameter of the theoretical.

```{r}
thetaPostMC = rbeta(n=1e6, 90, 610)
mean(thetaPostMC)
```

And the theoretical and data posteriors can be compared using a Q-Q plot

```{r}
qqplot(thetaPostEmp, thetaPostMC, type = "l", asp =1)
abline(a =0, b=1, col="blue")
```

So the emperical data-driven posterior theta is also a beta.

??? I DIDN'T GET THE END OF THIS SECTION 

JILL WILL DO

## 2.10 Nucleotide patterns

Exploring teh occurance of the Shine-Dalgarno sequence AGGAGGT motif in **Escherichia coli** K12-DH10B

```{r}
Ecoli
shineDalgarno = "AGGAGGT"
ecoli = Ecoli$NC_010473
```

We can count the SD instances in windows of 50000 bp using countPattern

```{r}
window = 50000
starts = seq(1, length(ecoli) - window, by = window)
ends = starts + window - 1
numMatches = vapply(seq_along(starts), function(i){
  countPattern(shineDalgarno, ecoli[starts[i]:ends[i]], max.mismatch=0)
}, numeric(1))
table(numMatches)
```

The occurance of sequence motifs along a sliding window most likely falls under a Poisson distribution.

```{r}
gf = goodfit(numMatches, "poisson")
summary(gf)
```

```{r}
distplot(numMatches, type="poisson")
```

The matches can be explored using matchPattern

```{r}
sdMatches = matchPattern(shineDalgarno, ecoli, max.mismatch = 0)
sdMatches
```

These are all 65 instances of the SD sequence. And now the distances between them...

```{r}
betweenmotifs = gaps(sdMatches)
betweenmotifs
```

So there are 65 SD sequences interrupting 66 regions of DNA. If the motif were occuring at random locations, the gap lengths would follow an exponential distribution.

```{r}
expplot(width(betweenmotifs), rate = 1/mean(width(betweenmotifs)), labels = "fit")
```

While the points follow the line for a bit, they deviate for the largest values, meaning they're less likely to be random / from an exponential distribution. Is this because of operons? Or perhaps phages/plasmids?

### 2.10.1 CpG islands and the chances of seeing dinucleotide sequences

CpG islands are regions of higher C and G content used in epigenetic regulation.

Let's look at human chromosome 8

```{r}
chr8 = Hsapiens$chr8
CpGtab = read.table("~/Desktop/stat7160/data/model-based-cpg-islands-hg19.txt", header = T)
nrow(CpGtab)
head(CpGtab)
```

```{r}
irCpG = with(dplyr::filter(CpGtab, chr == "chr8"), IRanges(start = start, end = end))

#filter subsets CpG tab to only include chromosome 8

grCpG = GRanges(ranges = irCpG, seqnames = "chr8", strand = "+")
genome(grCpG) = "hg19"
ideo = IdeogramTrack(genome = "hg19", chromosome = "chr8")
plotTracks(
  list(GenomeAxisTrack(),
       AnnotationTrack(grCpG, name = "CpG"), ideo),
  from = 2200000, to = 5800000, shape = "box", fill = "#006400", stacking = "dense"
)
```

We can then compute dinucleotide transition counts in both CpG and non-CpG regions.

```{r}
CGIview = Views(unmasked(Hsapiens$chr8), irCpG)
NonCGIview = Views(unmasked(Hsapiens$chr8), gaps(irCpG))
seqCGI = as(CGIview, "DNAStringSet")
seqNonCGI = as(NonCGIview, "DNAStringSet")
dinucCpG = sapply(seqCGI, dinucleotideFrequency)
dinucNonCpG = sapply(seqNonCGI, dinucleotideFrequency)
dinucNonCpG[,1]
```

```{r}
NonICounts = rowSums(dinucNonCpG)
IslCounts = rowSums(dinucCpG)
TI = matrix(IslCounts, ncol=4, byrow = T)
TnI = matrix(NonICounts, ncol=4, byrow =T)
dimnames(TI) = dimnames(TnI) = list(c("A", "C", "G", "T"), c("A", "C", "G", "T"))
MI = TI / rowSums(TI)
MI # a matrix of dinucleotide transition probabilities in CpG islands
```
```{r}
MN = TnI / rowSums(TnI)
MN # a matrix of dinucleotide transition probabilities not in CpG islands
```

Do base frequencies differ in and out of CpG islands?

```{r}
freqIsl = alphabetFrequency(seqCGI, baseOnly = T, collapse = T)[1:4]
freqIsl / sum(freqIsl)
```


```{r}
freqNon = alphabetFrequency(seqNonCGI, baseOnly = T, collapse = T)[1:4]
freqNon / sum(freqNon)
```

Given a sequence, we can determine the probability of it being within a CpG island or not using an *odds ratio* score.

??I DONT GET THIS PART BELOW

JER WILL DO

```{r}
alpha = log((freqIsl/sum(freqIsl)) / (freqNon/sum(freqNon)))
beta  = log(MI / MN)
```

```{r}
x = "ACGTTATACTACG"
scorefun = function(x) {
  s = unlist(strsplit(x, ""))
  score = alpha[s[1]]
  if (length(s) >= 2)
    for (j in 2:length(s))
      score = score + beta[s[j-1], s[j]]
  score
}
scorefun(x)
```

```{r}
generateRandomScores = function(s, len = 100, B = 1000) {
  alphFreq = alphabetFrequency(s)
  isGoodSeq = rowSums(alphFreq[, 5:ncol(alphFreq)]) == 0
  s = s[isGoodSeq]
  slen = sapply(s, length)
  prob = pmax(slen - len, 0)
  prob = prob / sum(prob)
  idx  = sample(length(s), B, replace = TRUE, prob = prob)
  ssmp = s[idx]
  start = sapply(ssmp, function(x) sample(length(x) - len, 1))
  scores = sapply(seq_len(B), function(i)
    scorefun(as.character(ssmp[[i]][start[i]+(1:len)]))
  )
  scores / len
}
scoresCGI    = generateRandomScores(seqCGI)
scoresNonCGI = generateRandomScores(seqNonCGI)
```

```{r}
br = seq(-0.8, 0.8, length.out = 50)
h1 = hist(scoresCGI,    breaks = br, plot = FALSE)
h2 = hist(scoresNonCGI, breaks = br, plot = FALSE)
plot(h1, col = rgb(0, 0, 1, 1/4), xlim = c(-0.5, 0.5), ylim=c(0,120))
plot(h2, col = rgb(1, 0, 0, 1/4), add = TRUE)
```



# Exercises

## 2.1 Generate 1,000 random 0/1 variables that model mutations occurring along a 1,000 long gene sequence. These occur independently at a rate of e-4 each. Then sum the 1,000 positions to count how many mutations in sequences of length 1,000.

Find the correct distribution for these mutation sums using a goodness of fit test and make a plot to visualize the quality of the fit.

## 2.2 Make a function that generates n random uniform numbers between 0 and 7 and returns their maximum.

Execute the function for n=25
Repeat this procedure B = 100 times.
Plot the distribution of these maxima.
What is the maximum likelihood estimate of the maximum of a sample of size 25?
Can you find a theoretical justification and the true maximum theta?


## 2.3 

a) Explore the data mtb using table to tabulate the AmAcid and Codon variables.
b) How was the PerThous variable created?
c) Write an R function that you can apply to the table to find which of the amino acids shows the strongest codon bias, i.e., the strongest departure from uniform distribution among its possible spellings.

```{r}
mtb = read.table("/Users/jeremiahyarmie/Desktop/stat7160/data/M_tuberculosis.txt", header = TRUE)
head(mtb, n = 4)
```

```{r}
#pro  =  mtb[ mtb$AmAcid == "Pro", "Number"]
#pro/sum(pro)

```

```{r}
mtb$AmAcid %>% table

```

```{r}
mtb$Codon %>% table
```

```{r}
mtb$Codon %>% table %>%  sum
```

## 2.4 Display GC content in a running window along the sequence of Staphylococcus aureus. 

a) Look at the complete staph object and then display the first three sequences in the set.
b) Find the GC content in tsequence windows of width 100.
c) Display the GC content in a sliding window as a fraction.
d) How could we visualize the overall trends of these proportions along the sequence?

```{r}
staph = readDNAStringSet("/Users/jeremiahyarmie/Desktop/stat7160/data/staphsequence.ffn.txt", "fasta")
```

## 2.5 Redo a figure similar to Figure 2.17, but include two other distributions: the uniform (which is B(1,1)) and the B(1/2, 1/2).



## 2.6 Re-analyse the data from Section 2.9.2 using a sketched prior.
